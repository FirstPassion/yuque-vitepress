## 自定义词嵌入层
这一层也是会被训练过程优化

```plsql
import torch
from torch import nn


class MyEmbedding(nn.Module):
    """
    词嵌入层
    :param vocab_size: 词典大小
    :param dim: 嵌入维度
    """

    def __init__(self, vocab_size, dim):
        super(MyEmbedding, self).__init__()
        # 将生成的矩阵注册为模型的参数,使其可以在训练过程中被优化
        self.emb_matrix = nn.Parameter(
            # 生成一个大小为[vocab_size, dim]的矩阵,矩阵中的每个元素都是0到vocab_size*dim之间的浮点数
            torch.arange(0, vocab_size * dim, dtype=torch.float32).reshape(
                vocab_size, dim
            )
        )

    def forward(self, ids):
        # 取出ids中每个元素的嵌入向量
        return self.emb_matrix[ids]

# 创建一个词嵌入层 词表大小为10 嵌入维度为3
emb = MyEmbedding(10, 3)
# 输入一个包含4个元素的张量
print(emb(torch.tensor([0, 1, 2, 4])))
```

## 输出结果
通过`ids`去取出下标处的嵌入向量，每一行对应一个词，`ids`就是一句话，取出来的2维矩阵就是一句话

```plsql
tensor([[ 0.,  1.,  2.],
        [ 3.,  4.,  5.],
        [ 6.,  7.,  8.],
        [12., 13., 14.]], grad_fn=<IndexBackward0>)
```

